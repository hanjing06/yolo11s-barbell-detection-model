{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanjing06/yolo11s-barbell-detection-model/blob/main/YOLO_Barbell_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Guide On How To Train A YOLO Model To Detect Objects (Oriented Bounding Boxes Object Detection).\n",
        "\n",
        "<small>(Specifically our barbell detection mode)</small>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Author**: Hanjing Lin and Mats Leis\n",
        "\n",
        "**Last Updated**: Friday Feburary 13 2026\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "###Introduction:\n",
        "\n",
        "This notebook uses Ultralytics to train the YOLO11 object detection model using our fixed dataset from the [RoboFlow Universe](https://universe.roboflow.com/techtitans-pcchf/barbell-detection-8phtm/dataset/12). At the end of this Colab, you can run our custom YOLO model that can run on your PC, phone, or edge device like a Raspberry Pi\n",
        "\n",
        "### Working in Colab\n",
        "Colab provides a virtual machine in your browser complete with a Linux OS, filesystem, Python environment, and best of all, a free GPU. We'll install PyTorch and Ultralytics in this environment and use it to train our model. Simply click the Play button on sections of code in this notebook to execute them on the virtual machine."
      ],
      "metadata": {
        "id": "LAwI2gPAyzSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Verifying the GPU availability\n",
        "\n",
        "Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting one of the GPU options (I'd recomment T4 GPU) in the Hardware accelerator section. **Click Play on the following code block below to verify that the NVIDIA GPU is present and ready for training.**\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://raw.githubusercontent.com/hanjing06/yolo11s-barbell-detection-model/refs/heads/main/docs/Runtime_Config.png\" height=\"360\"><br>\n",
        "<i>Figure 1: Recommended settings for runtime.</i>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "1TggVjf00taz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OKReOJkD6b8n",
        "outputId": "62363451-97a3-4e95-c6a3-dfeb9d6029ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb 13 07:32:12 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Training our model\n",
        "\n",
        "*Note that we have collected an already annotated and labeled dataset with over 500+ images from Roboflow*\n",
        "\n",
        "*However, if you'd like to gather and label your own images, check the note at the end*\n",
        "\n",
        "To start, make sure that your dataset folder has the same structure has Figure 2, and that you have a `classes.txt` file that contains all the classes\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://raw.githubusercontent.com/hanjing06/yolo11s-barbell-detection-model/refs/heads/main/docs/Folder_Structure.png\" height=\"300\"><br>\n",
        "<i>Figure 2: Folder structure, you can\n",
        "<a href=\"https://github.com/hanjing06/yolo11s-barbell-detection-model/blob/3e668770eba20b934fb2c786e923721aa748acb7/docs/barbellData.zip\" download=\"barbellData.zip\">download the zip</a> from the GitHub as an example.</i>\n",
        "</p><br>\n",
        "\n",
        "Once you've got your dataset built, put into the file structure shown above, and zipped into `barbellData.zip`, you're ready to move on to the next step."
      ],
      "metadata": {
        "id": "wT9Fg_zy6D49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Training the model\n",
        "\n",
        "Next, we'll upload our dataset (2.1) and prepare it for training with YOLO. We'll split the dataset into train and validation folders (2.2), and we'll automatically generate the configuration file for training the model.\n",
        "\n",
        "*Note: If your Roboflow dataset already came with train and validation folders, you can skip 2.2.*"
      ],
      "metadata": {
        "id": "XA36wn4NACGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 Uploading the zip to Colab\n",
        "\n",
        "We now have to upload the `barbellData.zip` file into Colab as shown in Figure 3.\n",
        "\n",
        "<p align=center><img src=\"https://raw.githubusercontent.com/hanjing06/yolo11s-barbell-detection-model/refs/heads/main/docs/Upload_Colab_Zip.png\"><i>Figure 3: How to upload a zip to Colab</p>"
      ],
      "metadata": {
        "id": "8eyjIaE8AjXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 Split images into testing and validation folders\n",
        "\n",
        "Next line unzips the data files, and creates a folder for it. Play the code below to extract.\n",
        "\n",
        "*Note: for personal use/modifications, you can change the file name to match the one you're using*"
      ],
      "metadata": {
        "id": "ATOzGg2RDKtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/barbellData.zip -d /content/barbellData"
      ],
      "metadata": {
        "id": "x0bTnQi96zuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultralytics requires a particular folder structure to store training data for models. You have your root folder, then two main folders inside:\n",
        "\n",
        "**Train:** Actual images to train the model. They train in epochs. One epoch of training means every image in the train set is passed into the <a href=\"https://www.geeksforgeeks.org/deep-learning/neural-networks-a-beginners-guide/\">neural network</a>. The training algorithm adjusts the network weights to fit the data in the images.\n",
        "\n",
        "**Validation:** Images are used to check model's performance at the end of each training epoch.\n",
        "\n",
        "*Note that epoch stands for how long the model trains for. For example: 60 epochs, it'll train with 60 images first.*\n",
        "\n",
        "It is recommended that for a dataset with less than 200 images, 60 epochs should be good, more than 200, 40 is good\n",
        "\n",
        "*Another note: you can change the model in the line below to adjust your training size/accuracy*\n",
        "\n",
        "Run the python code below made by <a href=\"https://www.linkedin.com/in/evan-juras/\">Evan Juras</a> to create a required folder of 90% of the images going into a training folder, and 10% of the images in the validations folder. (These folders will be in the root called \"data\")"
      ],
      "metadata": {
        "id": "Cl8eTvbs7ccP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/train_val_split.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py\n",
        "!python train_val_split.py --datapath=\"/content/barbellData\" --train_pct=0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir82ZvqI-B9V",
        "outputId": "8d2a2e45-ec46-49dd-d77b-4275bfa7e2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-13 17:49:05--  https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3203 (3.1K) [text/plain]\n",
            "Saving to: ‚Äò/content/train_val_split.py‚Äô\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/train_val_ 100%[===================>]   3.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-13 17:49:05 (62.1 MB/s) - ‚Äò/content/train_val_split.py‚Äô saved [3203/3203]\n",
            "\n",
            "Number of image files: 262\n",
            "Number of annotation files: 262\n",
            "Images moving to train: 235\n",
            "Images moving to validation: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Installing Ultralytics\n",
        "\n",
        "Next, we'll install the Ultralytics library in this Google Colab instance. This Python library will be used to train the YOLO model."
      ],
      "metadata": {
        "id": "IjcXHg6WGJo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9VSUEQrs6THh",
        "outputId": "f62afe98-b572-451f-85e5-0109a258d2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.14-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.92)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu128)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.14-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.14 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Configuring the training (Pre-training)\n",
        "\n",
        "Last step before running the training. We need to create a YAML file. This file specifies the location of your train and validation data, and it also defines the model's classes. An example configuration file model is available [here](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco128.yaml).\n",
        "\n",
        "To generate a YAML file. Run the python script below."
      ],
      "metadata": {
        "id": "X2mjNf5bGShE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python function to automatically create data.yaml config file\n",
        "# 1. Reads \"classes.txt\" file to get list of class names\n",
        "# 2. Creates data dictionary with correct paths to folders, number of classes, and names of classes\n",
        "# 3. Writes data in YAML format to data.yaml\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
        "\n",
        "  # Read class.txt to get class names\n",
        "  if not os.path.exists(path_to_classes_txt):\n",
        "    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n",
        "    return\n",
        "  with open(path_to_classes_txt, 'r') as f:\n",
        "    classes = []\n",
        "    for line in f.readlines():\n",
        "      if len(line.strip()) == 0: continue\n",
        "      classes.append(line.strip())\n",
        "  number_of_classes = len(classes)\n",
        "\n",
        "  # Create data dictionary\n",
        "  data = {\n",
        "      'path': '/content/data',\n",
        "      'train': 'train/images',\n",
        "      'val': 'validation/images',\n",
        "      'nc': number_of_classes,\n",
        "      'names': classes\n",
        "  }\n",
        "\n",
        "  # Write data to YAML file\n",
        "  with open(path_to_data_yaml, 'w') as f:\n",
        "    yaml.dump(data, f, sort_keys=False)\n",
        "  print(f'Created config file at {path_to_data_yaml}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Define path to classes.txt and run function\n",
        "path_to_classes_txt = '/content/barbellData/classes.txt'\n",
        "path_to_data_yaml = '/content/data.yaml'\n",
        "\n",
        "create_data_yaml(path_to_classes_txt, path_to_data_yaml)\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "!cat /content/data.yaml"
      ],
      "metadata": {
        "id": "4letvP7X12ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1541a9ae-5b7d-4e35-f7f4-af9fe4cef5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created config file at /content/data.yaml\n",
            "\n",
            "File contents:\n",
            "\n",
            "path: /content/data\n",
            "train: train/images\n",
            "val: validation/images\n",
            "nc: 3\n",
            "names:\n",
            "- barbell\n",
            "- plate\n",
            "- person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Training the model\n",
        "\n",
        "####5.1 Parameters\n",
        "\n",
        "We are training the model on Colab which uses a cloud-based GPU but you can nonetheless <a href=\"https://docs.ultralytics.com/modes/train/\">train locally.</a>\n",
        "\n",
        "<br>Below are some learnings about machine learning (yay!)\n",
        "\n",
        "####5.1.1 Choosing a model\n",
        "\n",
        "<p align=center><a href=\"https://docs.ultralytics.com/compare/#interactive-performance-benchmarks\"><img src=\"https://cdn.prod.website-files.com/680a070c3b99253410dd3df5/684d853f09317da8ad90cd28_67ed50e66edd3138c81f2636_67dd6e628e9151bd18fdb88a_rcnn_fig6.webp\"></a><i>Figure 4: Performance Benchmarks. <a href=\"https://docs.ultralytics.com/compare/#quick-decision-guide\">Check out their quick decision guide here.</a></i></p><br>\n",
        "\n",
        "We are using the `yolo11s.pt` model\n",
        "\n",
        "(Recall) Number of epochs is number of \"pass throughs\", it correlates to how long the model will train for. A good rule of thumb is: > 200 images = 60 epochs, < 200 images = 40 epochs\n",
        "\n",
        "Resolution of the image has a large impact as it affects speed and accuracy. Higher res = slower but more accurate, lower res = faster but less accurateYOLO models are typically trained and inferenced at a 640x640 resolution. However, if you want your model to run faster or know you will be working with low-resolution images, try using a lower resolution like 480x480. If you wanna learn why this is a case, I'd recommend this video at 2:45: <a href=\"https://youtu.be/aircAruvnKk?si=jBqO1Ce5bRHGaXno&t=165\">\n",
        "But what is a neural network? | Deep learning chapter 1</a> By 3Blue1Brown"
      ],
      "metadata": {
        "id": "xcIRaBLMH0wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2 Run the training\n",
        "\n",
        "Run the following code block to begin training. If you want to use a different model, number of epochs, or resolution, change model, epochs, or imgsz.\n",
        "\n",
        "The default will be the `yolo11s.pt` model at `60 epochs` with a `640x640` resolution"
      ],
      "metadata": {
        "id": "U7i-Wt5kP_dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=60 imgsz=640"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6017rYE80wQ",
        "outputId": "f4d709a0-8fb9-4734-b919-eb6e4fa7aac2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.14 üöÄ Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=60, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 691, in get_dataset\n",
            "    data = check_det_dataset(self.args.data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/data/utils.py\", line 462, in check_det_dataset\n",
            "    raise FileNotFoundError(m)\n",
            "FileNotFoundError: Dataset '/content/data.yaml' images not found, missing path '/content/valid/images'\n",
            "Note dataset download directory is '/content/datasets'. You can update this in '/root/.config/Ultralytics/settings.json'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 986, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 769, in train\n",
            "    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\", line 63, in __init__\n",
            "    super().__init__(cfg, overrides, _callbacks)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 186, in __init__\n",
            "    self.data = self.get_dataset()\n",
            "                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 695, in get_dataset\n",
            "    raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error ‚ùå {e}\")) from e\n",
            "RuntimeError: Dataset '/content/data.yaml' error ‚ùå Dataset '/content/data.yaml' images not found, missing path '/content/valid/images'\n",
            "Note dataset download directory is '/content/datasets'. You can update this in '/root/.config/Ultralytics/settings.json'\n",
            "Sentry is attempting to send 2 pending events\n",
            "Waiting up to 2 seconds\n",
            "Press Ctrl-C to quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the console is doing is:\n",
        "1. parsing the images in the training and validation directories.\n",
        "2. training the model.\n",
        "3. during each epoch cycle, runs the model on the validation dataset and reports the resulting <a href=\"https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173\">mAP</a>, precision, and recall.\n",
        "4. the program will end once the number of epochs is reached (default case: 60 epochs)\n",
        "\n",
        "Then, once the training is complete, the best trained model weights will be saved in `content/runs/detect/train/weights/best.pt`. Information about training in the `content/runs/detect/train` will have a `results.png` this file will demonstrate how loss, precision, recall, and mAP progressed over each epoch."
      ],
      "metadata": {
        "id": "70hkxN8EQg5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. The Test Model\n",
        "\n",
        "Now the model has been trained, let's test it! Run the command below, it'll run the model on the images in the validation folder and then display the results for the first 10 images. This is a good way to confirm your model is working as expected. Click Play on the blocks below to see how your model performs."
      ],
      "metadata": {
        "id": "wGaQUvg9R76Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=data/validation/images save=True"
      ],
      "metadata": {
        "id": "Z3pSZmejSS2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "pm3hPAlwSW9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model should be drawing boxes, the same thing you would've done in Label Studio. If it is not doing a great job, you might need to increase the epochs, use a larger model, add more images to the training dataset or double check there is no labelling errors.\n"
      ],
      "metadata": {
        "id": "wgxuryceSbQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Deplying the model (How to download your newly trained model and run it on a local device)\n",
        "\n",
        "Now that your custom model has been trained, it's ready to be downloaded and deployed in an application! YOLO models can run on a wide variety of hardware, including PCs, embedded systems, and phones. Ultralytics makes it easy to convert the YOLO models to various formats (`tflite`, `onnx`, etc.) and deploy them in a variety of environments."
      ],
      "metadata": {
        "id": "jmHHWOyHkmMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.1 Download YOLO Model\n",
        "\n",
        "1. Zip and download the trained model by running the code block below.\n",
        "2. It'll create a folder called `barbell_detection_model`.\n",
        "3. Moves the model weights into it.\n",
        "4. Renames `best.pt` to `barbell_detection_model.pt`.\n",
        "5. Adds the training results to reference for later.\n",
        "6. Zips the whole thing."
      ],
      "metadata": {
        "id": "xnzKqfbHk_kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create \"model_name\" folder to store model weights and train results\n",
        "!mkdir /content/barbell_detection_model\n",
        "!cp /content/runs/detect/train/weights/best.pt /content/barbell_detection_model/barbell_detection_model.pt\n",
        "!cp -r /content/runs/detect/train /content/barbell_detection_model\n",
        "\n",
        "# Zip into \"the .zip file\"\n",
        "%cd barbell_detection_model\n",
        "!zip /content/barbell_detection_model.zip barbell_detection_model.pt\n",
        "!zip -r /content/barbell_detection_model.zip train\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "3YtlEXa_l8IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.2 Run the model locally\n",
        "\n",
        "For python on your local device, <a href=\"https://www.linkedin.com/in/evan-juras/\">Evan Juras</a> wrote this <a href=\"https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/yolo_detect.py\">python script</a> that loads the model, run inference on an image, parse the inference results, and displays boxes around each detected class in the image.\n",
        "\n",
        "You can download using https://www.anaconda.com/download but for this, we are just going to `pip install`. Below are simple steps to setting up a project directory and setitng up a python environment in the bash terminal.\n",
        "\n",
        "<table><tr><td width=\"50%\">\n",
        "\n",
        "**macOS/Linux**\n",
        "```bash\n",
        "mkdir project_name\n",
        "cd project_name\n",
        "python -m venv venv\n",
        "source venv/bin/activate\n",
        "```\n",
        "\n",
        "</td>\n",
        "<td width=\"50%\">\n",
        "\n",
        "**Windows**\n",
        "```bash\n",
        "mkdir project_name\n",
        "cd project_name\n",
        "python -m venv venv\n",
        "venv\\Scripts\\activate\n",
        "```\n",
        "\n",
        "</td></tr></table>\n",
        "\n",
        "\n",
        "1. Install Ultralytics using the terminal\n",
        "\n",
        "```bash\n",
        "  pip install -U ultralytics\n",
        "```\n",
        "\n",
        "2.Download the YOLO model script and put it into the project root\n",
        "\n",
        "      curl -o yolo-detector.py /path/to/projectroot\n",
        "\n",
        "To run inference with a yolov8s model on a USB camera at 1280x720 resolution, issue:\n",
        "\n",
        "      python yolo_detect.py --model barbell_detection_model.pt --source usb0 --resolution 1280x720\n",
        "\n",
        "If you are having trouble setting up your IDE, are missing packages, or want to install using Conda or Docker, instead of pip, refer to the Ultralytics <a href=\"https://docs.ultralytics.com/quickstart/#conda-docker-image\">intallation guide</a>.\n",
        "\n"
      ],
      "metadata": {
        "id": "V3NCdrJLm5qT"
      }
    }
  ]
}